{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HODDIES Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to HODDIES and Halo Occupation Distribution (HOD) Models\n",
        "\n",
        "HODDIES is a Python package designed to compute Halo Occupation Distribution (HOD) models and generate mock galaxy catalogues. It provides functionalities to link galaxies to dark matter halos based on various HOD formalisms.\n",
        "\n",
        "### What is the Halo Occupation Distribution (HOD)?\n",
        "\n",
        "The Halo Occupation Distribution (HOD) is a statistical framework used in cosmology to describe how galaxies populate dark matter halos. It specifies the probability that a dark matter halo of a given mass $M$ contains a certain number of galaxies. The HOD is typically broken down into two components:\n",
        "\n",
        "1.  **Central Galaxies**: The probability of a halo hosting a central galaxy. Each halo is assumed to host at most one central galaxy.\n",
        "2.  **Satellite Galaxies**: The mean number of satellite galaxies in a halo, given that it hosts a central galaxy. These satellites are typically distributed within the halo following a certain profile (e.g., NFW profile).\n",
        "\n",
        "The total mean number of galaxies in a halo of mass $ M $ is given by:\n",
        "\n",
        "$$\\langle N_{\\text{gal}}(M) \\rangle = \\langle N_{\\text{cen}}(M) \\rangle + \\langle N_{\\text{sat}}(M) \\rangle$$\n",
        "\n",
        "### HOD Models in HODDIES\n",
        "\n",
        "The `HODDIES/HODDIES/HOD_models.py` file implements several HOD models. Here's a brief overview of some of them:\n",
        "\n",
        "#### a) Standard HOD (SHOD) - (Zheng et al. 2007)\n",
        "\n",
        "This model is commonly used and describes the mean number of central galaxies with a softened step function, and satellite galaxies with a power law.\n",
        "\n",
        "**Central Galaxy Occupation:**\n",
        "$$\\langle N_{\\text{cen}}(M) \\rangle = \\frac{1}{2} \\left[ 1 + \\mathrm{erf} \\left( \\frac{\\log_{10} M - \\log_{10} M_{\\text{min}}}{\\sigma_{\\log M}} \\right) \\right]$$\n",
        "Where:\n",
        "- $M$: Halo mass\n",
        "- $M_{\\text{min}}$: Characteristic minimum halo mass for a central galaxy\n",
        "- $\\sigma_{\\log M}$: Width of the transition in central occupation\n",
        "\n",
        "**Satellite Galaxy Occupation:**\n",
        "$$\\langle N_{\\text{sat}}(M) \\rangle = A_s \\left( \\frac{M - M_0}{M_1} \\right)^\\alpha$$\n",
        "Where:\n",
        "- $M_0$: Mass threshold below which no satellites form\n",
        "- $M_1$: Characteristic mass scale for hosting one satellite galaxy\n",
        "- $\\alpha$: Power-law slope for satellite occupation\n",
        "- $A_s$: Normalization amplitude for satellites\n",
        "\n",
        "The implementation in `HOD_models.py` for `SHOD` (lines 164-186) corresponds to the central occupation, and `Nsat_pow_law` (lines 188-215) corresponds to the satellite occupation.\n",
        "\n",
        "#### b) Gaussian HOD (GHOD) - (arXiv:1708.07628)\n",
        "\n",
        "This model uses a Gaussian function to describe the central galaxy occupation.\n",
        "\n",
        "$$\\langle N_{\\text{cen}}(M) \\rangle = \\frac{A_c}{\\sqrt{2 \\pi} \\sigma_M} \\exp\\left(-\\frac{(\\log_{10} M - M_c)^2}{2 \\sigma_M^2}\\right)$$\n",
        "Where:\n",
        "- $M_c$: Mean halo mass (log10)\n",
        "- $\\sigma_M$: Width of central galaxy mass distribution\n",
        "- $A_c$: Normalization amplitude\n",
        "\n",
        "(See `GHOD` function in `HOD_models.py`, lines 75-99)\n",
        "\n",
        "#### c) Log-normal HOD (LNHOD) - (arXiv:2306.06319)\n",
        "\n",
        "The LNHOD model, based on arXiv:2306.06319, uses a log-normal distribution for central galaxy occupation. The function returns 0 if $x \\le 0$, otherwise:\n",
        "\n",
        "$$\\langle N_{\\text{cen}}(M) \\rangle = \\frac{A_c}{x \\sigma_M \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(\\log(x))^2}{2 \\sigma_M^2}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $M$: Halo mass (represented as `log10_Mh` in the code, and $x$ is derived from it).\n",
        "- $A_c$: Normalization amplitude (`Ac`).\n",
        "- $M_c$: Mean halo mass (log10) (`Mc`).\n",
        "- $\\sigma_M$: Width of central galaxy mass distribution (`sig_M`).\n",
        "- $x = 10^{\\log_{10} M} - 10^{M_c} + 1$ (approximated as `log10_Mh - Mc + 1` in the code for the log-transformed mass difference, where `log10_Mh` and `Mc` are already log values).\n",
        "\n",
        "(See `LNHOD` function in `HOD_models.py`, lines 101-129)\n",
        "\n",
        "#### d) Star-forming HOD (SFHOD) - (arXiv:1708.07628)\n",
        "\n",
        "The SFHOD model, based on arXiv:1708.07628, has a piecewise definition for central galaxy occupation:\n",
        "\n",
        "If $M_c \\ge \\log_{10} M$:\n",
        "$$\\langle N_{\\text{cen}}(M) \\rangle = \\frac{A_c}{\\sqrt{2 \\pi} \\sigma_M} \\exp\\left(-\\frac{(\\log_{10} M - M_c)^2}{2 \\sigma_M^2}\\right)$$\n",
        "\n",
        "Else (if $\\log_{10} M > M_c$):\n",
        "$$\\langle N_{\\text{cen}}(M) \\rangle = \\frac{A_c}{\\sqrt{2 \\pi} \\sigma_M} \\left( \\frac{10^{\\log_{10} M}}{10^{M_c}} \\right)^\\gamma$$\n",
        "\n",
        "Where:\n",
        "- $M$: Halo mass (represented as `log10_Mh`).\n",
        "- $A_c$: Normalization amplitude (`Ac`).\n",
        "- $M_c$: Mean halo mass (log10) (`Mc`).\n",
        "- $\\sigma_M$: Width of central galaxy mass distribution (`sig_M`).\n",
        "- $\\gamma$: Shape parameter controlling the asymmetry (`gamma`).\n",
        "(See `SFHOD` function in `HOD_models.py`, lines 131-161)\n",
        "\n",
        "#### e) High Mass Quenched (HMQ) and modified HMQ (mHMQ) - (arXiv:1910.05095, arXiv:2306.06319)\n",
        "\n",
        "These are more complex models, particularly useful for distinguishing between quenched and star-forming galaxy populations. HMQ includes a quenching factor (Q) and a maximum probability (pmax). mHMQ is a simplified version without quenching terms.\n",
        "\n",
        "The HMQ model, based on arxiv:1910.05095, describes the expected number of galaxies as:\n",
        "\n",
        "$$\\text{HMQ}(\\log_{10} M_h) = A_c \\left( 2 A \\phi_x \\Phi_{\\gamma,x} + \\frac{0.5}{Q} \\left( 1 + \\mathrm{erf}\\left(\\frac{\\log_{10} M_h - M_c}{0.01}\\right) \\right) \\right)$$\n",
        "\n",
        "Where:\n",
        "- $\\log_{10} M_h$: Logarithm (base 10) of halo mass.\n",
        "- $A_c$: Normalization amplitude (`Ac`).\n",
        "- $M_c$: Mean halo mass (log10) (`Mc`).\n",
        "- $\\sigma_M$: Width of central galaxy mass distribution (`sig_M`).\n",
        "- $\\gamma$: Shape parameter controlling the asymmetry (`gamma`).\n",
        "- $Q$: Quenching factor (`Q`).\n",
        "- $pmax$: Maximum probability (`pmax`).\n",
        "- $A = (pmax - 1/Q)$.\n",
        "- $\\phi_x = \\frac{1}{\\sqrt{2 \\pi} \\sigma_M} \\exp\\left(-\\frac{(\\log_{10} M_h - M_c)^2}{2 \\sigma_M^2}\\right)$.\n",
        "- $\\Phi_{\\gamma,x} = 0.5 \\left( 1 + \\mathrm{erf}\\left(\\frac{\\gamma (\\log_{10} M_h - M_c)}{\\sigma_M \\sqrt{2}}\\right) \\right).\n",
        "\n",
        "(See `HMQ` and `mHMQ` functions in `HOD_models.py`, lines 6-41 and 43-73 respectively)\n",
        "\n",
        "The specific parameters for each HOD model are defined within the functions in `HOD_models.py`, and their interpretation will depend on the chosen model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Parameter File and Running the Code\n",
        "\n",
        "HODDIES uses a YAML-formatted parameter file to configure the HOD model, simulation settings, and analysis parameters. This file acts as a central control for your simulations and analyses. Let's break down the key sections of a typical parameter file.\n",
        "\n",
        "### Parameter File Structure (e.g., `HODDIES/default_HOD_parameters.yaml`)\n",
        "\n",
        "A parameter file in HODDIES generally includes the following sections:\n",
        "\n",
        "#### a) `tracers`\n",
        "\n",
        "This section lists the galaxy populations (tracers) you want to model. Each tracer will have its own set of HOD parameters defined subsequently. For exemple:\n",
        "\n",
        "```\n",
        "tracers:\n",
        "  - LRG\n",
        "  - ELG\n",
        "  - QSO\n",
        "```\n",
        "\n",
        "#### b) HOD Model Parameters (e.g., `LRG`, `ELG`, `QSO` sections)\n",
        "\n",
        "For each tracer defined in the `tracers` section, there will be a corresponding section detailing its HOD parameters. These parameters govern the central and satellite galaxy occupations within halos.\n",
        "\n",
        "**Central Galaxy Parameters:**\n",
        "- `HOD_model`: Specifies the HOD model for central galaxies (e.g., `SHOD`, `GHOD`, `HMQ`, `mHMQ`, `LNHOD`, `SFHOD`).\n",
        "- `Ac`: Normalization amplitude.\n",
        "- `log_Mcent`: Characteristic minimum halo mass (log10) for central galaxies.\n",
        "- `sigma_M`: Width of the central galaxy mass distribution or steepness of the step function.\n",
        "- `gamma`, `Q`, `pmax`: Additional parameters depending on the chosen `HOD_model` (e.g., `HMQ`, `mHMQ`, `SFHOD`).\n",
        "\n",
        "**Satellite Galaxy Parameters:**\n",
        "- `satellites`: Boolean, `true` if satellite galaxies are included.\n",
        "- `sat_HOD_model`: HOD model for satellite galaxies (e.g., `Nsat_pow_law`).\n",
        "- `As`: Normalization amplitude for satellites.\n",
        "- `M_0`, `M_1`, `alpha`: Parameters for the satellite power-law model.\n",
        "- `f_sigv`: Scaling factor for velocity dispersion.\n",
        "- `vel_sat`: Velocity distribution model for satellites (e.g., `rd_normal`, `NFW`, `infall`).\n",
        "- `density`: Target galaxy number density.\n",
        "- `vsmear`: Redshift error smearing. Can be a float for a fixed value or a list `[zmin, zmax]` to use DESI redshift error distribution.\n",
        "\n",
        "**Other Parameters:**\n",
        "- `assembly_bias`: Parameters related to assembly bias.\n",
        "- `conformity_bias`: Boolean, to include conformity bias.\n",
        "- `exp_frac`, `exp_scale`, `nfw_rescale`: Parameters for modified NFW profiles.\n",
        "\n",
        "#### c) `hcat` (Halo Catalog and Simulation Settings)\n",
        "\n",
        "This section configures the halo catalog from which mock galaxies will be generated.\n",
        "\n",
        "- `boxsize`: Simulation box size.\n",
        "- `path_to_sim`: Base path to halo catalogs.\n",
        "- `z_simu`: Redshift of the simulation snapshot.\n",
        "- `Abacus`: Specific settings for AbacusSummit simulations (e.g., `sim_name`, `load_particles`).\n",
        "- `Uchuu`: Specific settings for Uchuu simulations (e.g., `sim_name`, `path_to_sim`, `load_subhalos`).\n",
        "\n",
        "#### d) `2PCF_settings` (Two-Point Correlation Function Settings)\n",
        "\n",
        "These parameters control how the two-point correlation function (2PCF) is computed.\n",
        "\n",
        "- `rsd`: Boolean, whether to include redshift-space distortions.\n",
        "- `bin_logscale`: Boolean, whether to use logarithmic binning for `r`.\n",
        "- `multipole_index`: List of multipole moments to compute (e.g., `0` for monopole, `2` for quadrupole).\n",
        "- `rmax`, `rmin`, `rp_max`, `rp_min`: Radial and projected separation ranges.\n",
        "- `n_r_bins`, `n_rp_bins`, `n_mu_bins`: Number of bins for various correlation function calculations.\n",
        "- `pimax`: Maximum $\\pi$ for $w_p$ integration.\n",
        "\n",
        "#### e) `cosmo` (Cosmology Settings)\n",
        "\n",
        "This section allows you to define the cosmology, though it's often automatically set if using AbacusSummit simulations.\n",
        "\n",
        "- `fiducial_cosmo`: Name of a predefined cosmology (e.g., `Planck2018FullFlatLCDM`).\n",
        "- `engine`: Cosmology backend (e.g., `class`).\n",
        "- `Omega_m`, `Omega_cdm`, `Omega_L`, `Omega_b`, `sigma_8`, `n_s`, `w0_fdl`, `wa_fdl`: Cosmological parameters.\n",
        "\n",
        "#### f) Global Settings\n",
        "\n",
        "- `seed`: Global random seed for reproducibility.\n",
        "- `nthreads`: Number of threads to use for parallel processing.\n",
        "- `use_assembly_bias`, `use_particles`: Global flags.\n",
        "\n",
        "#### g) `fit_param` (Fitting Settings)\n",
        "\n",
        "This section is crucial for configuring the fitting and minimization procedures.\n",
        "\n",
        "- `nb_real`: Number of mock catalogs to generate for each HOD parameter set.\n",
        "- `fit_name`: Name identifier for this fitting run.\n",
        "- `path_to_training_point`: null  # Path to save or read precomputed training samples\n",
        "- `dir_output_fit`: path_to_save_fit_outputs  # Directory where output from this fitting will be saved\n",
        "- `fit_type`: wp+xi  # Type of 2PCF(s) used in fitting: wp = projected, xi = multipoles\n",
        "- `generate_training_sample`: True  # Whether to generate a new training sample using sampling_type\n",
        "- `sampling_type`: Hammersley  # Sampling method for generating initial training points. Choices: LHS or Hammersley \n",
        "- `N_trainning_points`: 800  # Number of training points to generate\n",
        "- `seed_trainning`: 18  # Random seed for reproducibility of LHS sampling\n",
        "- `n_calls`: 800  # Number of GP iterations (total number of sampling steps)\n",
        "- `logchi2`: True  # Use log(chi²) instead of raw chi² in GP modeling (recommended)\n",
        "\n",
        "- `sampler`: 'emcee'  # MCMC sampler to use ('emcee' or 'zeus')\n",
        "- `n_iter`: 10000  # Number of iterations for MCMC sampling\n",
        "- `nwalkers`: 20  # Number of walkers for the MCMC sampler\n",
        "- `func_aq`: 'EI'  # Acquisition function for GP-based optimization ('EI' = Expected Improvement)\n",
        "\n",
        "- `length_scale_bounds`: [0.001, 10]  # Bounds for GP kernel length scale optimization\n",
        "- `length_scale`: False  # If set to a list, GP kernel uses fixed length scales\n",
        "- `kernel_gp`: Matern_52  # Type of GP kernel to use\n",
        "\n",
        "- `save_fn`: 'results_fit.npy'\n",
        "\n",
        "- `use_desi_data`: True \n",
        "- `zmin`: 0.8\n",
        "- `zmax`: 1.1\n",
        "- `dir_data`: /global/homes/a/arocher/users_arocher/Y3/loa-v1/v2/PIP/cosmo_0\n",
        "- `region`: GCcomb\n",
        "- `weights_type`: pip_angular_bitwise\n",
        "- `njack`: 128\n",
        "- `nran`: 4\n",
        "- `bin_type`: log\n",
        "- `load_cov_jk`: False  # Use JK covariance matrix\n",
        "- `corr_dir`: /global/cfs/cdirs/desi/users/arocher/Y1/2PCF_for_corr/Abcaus_small_boxes/  #root directory to find correlation matrices (to be rescaled by JK variances)\n",
        "- `nb_mocks`: 1883 # Nb of mocks used to compute the covariance\n",
        "- `use_vsmear`: True\n",
        "- `fit_type`: wp+xi # Only wp or xi available at the moment\n",
        "\n",
        "- `priors`:  \n",
        "    LRG:\n",
        "      M_0: [12.5, 13.5]\n",
        "      M_1: [13, 14.5] \n",
        "      alpha: [0.5, 1.5]\n",
        "      f_sigv: [0.5, 1.5]\n",
        "      log_Mcent: [12.4, 13.5]\n",
        "      sigma_M: [0.05, 1]\n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Run the Code\n",
        "\n",
        "To run HODDIES, you typically import the `HOD` class and initialize it with your parameter file. Here's a basic example:\n",
        "\n",
        "```python\n",
        "import yaml\n",
        "from HODDIES import HOD\n",
        "\n",
        "# Load your parameter file\n",
        "params = yaml.load(open('your_parameter_file.yaml'), Loader=yaml.FullLoader)\n",
        "\n",
        "# Initialize the HOD object\n",
        "hod_obj = HOD(param_file='your_parameter_file.yaml')\n",
        "\n",
        "# You can then access and modify parameters directly if needed\n",
        "# hod_obj.args['QSO']['density'] = 0.0006\n",
        "\n",
        "# To make a mock catalog (more details in the next section)\n",
        "# mock_catalog = hod_obj.make_mock_cat()\n",
        "\n",
        "# To run the minimisation (more details later)\n",
        "# result = hod_obj.minimize_hod()\n",
        "```\n",
        "\n",
        "The `param_file` argument in `HOD()` constructor takes the path to your YAML parameter file. You can also pass a dictionary to the constructor directly if you've loaded it with `yaml.load()`.\n",
        "\n",
        "Now let's move on to generating mock catalogues and computing correlation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Loading Halo Catalogues, Creating Mock Catalogues, and Computing Correlation Functions\n",
        "\n",
        "This section will guide you through the process of loading dark matter halo catalogues, populating them with galaxies using an HOD model to create mock catalogues, and then computing two-point correlation functions (2PCF), specifically projected correlation function ($w_p$) and redshift-space multipoles ($\\xi_0, \\xi_2$).\n",
        "\n",
        "### a) Loading Halo Catalogues\n",
        "\n",
        "HODDIES can interface with different halo catalogues, such as AbacusSummit and Uchuu. The selection and configuration of the halo catalogue are done through the `hcat` section of your parameter file.\n",
        "\n",
        "Here's how you would typically initialize the `HOD` object with a specified halo catalogue. In this example, we'll use AbacusSummit.\n",
        "\n",
        "First, make sure to import the necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from HODDIES import HOD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's set up the HOD object, specifying the path to the Abacus simulation and the parameter file. You can also override parameters directly when initializing the `HOD` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set number of threads to 32\n",
            "Load Compaso cat from /global/cfs/cdirs/desi/cosmosim/Abacus/AbacusSummit_highbase_c000_ph100/halos/z0.950 ...\n",
            "Done took 00:00:27\n",
            "Compute columns...\n",
            "Done took  00:00:00\n",
            "AbacusSummit_highbase_c000_ph100 at 0.95 loaded, took 00:00:28\n",
            "Initialize Abacus c000 cosmology\n",
            "HOD object initialized successfully!\n",
            "Loaded Abacus simulation: AbacusSummit_highbase_c000_ph100 at z=0.95\n"
          ]
        }
      ],
      "source": [
        "# Define the redshift of the simulation\n",
        "z_sim = 0.95\n",
        "\n",
        "# Path to your Abacus simulation. Please adjust this path to your local setup.\n",
        "path_to_abacus = '/global/cfs/cdirs/desi/cosmosim/Abacus'\n",
        "\n",
        "# Load your parameter file\n",
        "param_file_path = 'HODDIES/desi/test_fit_param_QSO.yaml'\n",
        "# args = yaml.load(open(param_file_path), Loader=yaml.FullLoader)\n",
        "\n",
        "# Initialize the HOD object, overriding halo catalog settings for Abacus\n",
        "hod_obj = HOD(\n",
        "    path_to_abacus_sim=path_to_abacus,\n",
        "    param_file=param_file_path,\n",
        "    hcat={\n",
        "        'Abacus': {\n",
        "            'sim_name': 'AbacusSummit_highbase_c000_ph100',\n",
        "            'z_simu': z_sim\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"HOD object initialized successfully!\")\n",
        "print(f\"Loaded Abacus simulation: {hod_obj.args['hcat']['Abacus']['sim_name']} at z={hod_obj.args['hcat']['Abacus']['z_simu']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading Uchuu Halo Catalogues\n",
        "\n",
        "To load an Uchuu halo catalogue, you would modify the `hcat` section of your `HOD` object initialization. Here's an example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set number of threads to 32\n",
            "Load Uchuu Uchuu2Gpc simulation at z=0.940 \n",
            "Reading ['/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_02.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_04.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_03.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_07.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_05.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_00.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_01.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_06.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_09.h5', '/pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_08.h5']\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_02.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_04.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_03.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_07.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_05.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_00.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_01.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_06.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_09.h5\n",
            "Reading file /pscratch/sd/a/arocher/Uchuu/Uchuu_halo_catalogs/halodir_034/Uchuu_halodir_034_halos_subhalos_z0.940_08.h5\n",
            "Done 193.20\n",
            "HOD object initialized with Uchuu simulation successfully!\n",
            "Loaded Uchuu simulation: Uchuu2Gpc at z=0.94\n"
          ]
        }
      ],
      "source": [
        "# Define the redshift for Uchuu simulation (example)\n",
        "z_sim_uchuu = 0.94 # Uchuu snapshot [0.43, 0.49, 0.56, 0.63, 0.70, 0.78, 0.86, 0.94, 1.03, 1.12, 1.22, 1.32, 1.43, 1.65]\n",
        "\n",
        "\n",
        "# Re-initialize the HOD object for Uchuu, overriding halo catalog settings\n",
        "# You would typically do this in a separate notebook or clear previous HOD object\n",
        "hod_obj_uchuu = HOD(\n",
        "    param_file=param_file_path, # Using the same parameter file for HOD settings\n",
        "    hcat={'z_simu': z_sim_uchuu,\n",
        "        'Uchuu': { \n",
        "            'sim_name': 'Uchuu2Gpc', # Example Uchuu sim name Choices ['Planck18', 'Planck18_DDE', 'DESIY1_DEE', 'Uchuu2Gpc']  \n",
        "            'load_subhalos': False # Set to True to use subhalos\n",
        "        }\n",
        "    },\n",
        "    read_Uchuu=True,\n",
        "    nthreads=128\n",
        ")\n",
        "\n",
        "print(\"HOD object initialized with Uchuu simulation successfully!\")\n",
        "print(f\"Loaded Uchuu simulation: {hod_obj_uchuu.args['hcat']['Uchuu']['sim_name']} at z={hod_obj_uchuu.args['hcat']['z_simu']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Creating Mock Catalogues\n",
        "\n",
        "Once the `HOD` object is initialized with the halo catalogue, you can create a mock galaxy catalogue. The `make_mock_cat()` method populates the halos with central and satellite galaxies according to the HOD model specified in the parameter file.\n",
        "\n",
        "Before creating the mock catalogue, it's often useful to set the HOD parameters. In this example, we'll use some example HOD parameters. In a real scenario, these might come from a fitting procedure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create mock catalog for ['QSO']\n",
            "Run HOD for QSO\n",
            "Set density to 0.0005 gal/Mpc/h\n",
            "HOD Computed 6.220611095428467\n",
            "Start satellite assignement\n",
            "Satellite assignement done 0.953514814376831\n",
            "QSO mock catalogue done 1.630145788192749\n",
            "3250104 central galaxies, 746921 satellites, fraction of satellite 0.19 \n",
            "Done overall time  QSO 12.784969329833984\n",
            "Mock catalogue created successfully!\n",
            "Number of galaxies in mock: 3997025\n",
            "['Mh', 'Rh', 'Rs', 'Vrms', 'c', 'halo_id', 'log10_Mh', 'vx', 'vy', 'vz', 'x', 'y', 'z', 'Central', 'TRACER']\n"
          ]
        }
      ],
      "source": [
        "# Example HOD parameters (these would typically come from a fit)\n",
        "# Using parameters from QSO_fits.ipynb for z=0.95\n",
        "ex_hod_params = {\n",
        "    'M_0': 11.93, \n",
        "    'M_1': 13.29, \n",
        "    'alpha': 0.84, \n",
        "    'f_sigv': 0.99, \n",
        "    'log_Mcent': 12.17, \n",
        "    'sigma_M': 0.37,\n",
        "    'density': 0.0005 # Example density from test_fit_param_QSO.yaml\n",
        "}\n",
        "\n",
        "# Update the HOD object's parameters for the QSO tracer\n",
        "hod_obj_uchuu.args['QSO'].update(ex_hod_params)\n",
        "\n",
        "# Create the mock catalogue\n",
        "mock_catalogue = hod_obj_uchuu.make_mock_cat(fix_seed=42) # fix_seed for reproducibility\n",
        "\n",
        "print(\"Mock catalogue created successfully!\")\n",
        "print(f\"Number of galaxies in mock: {len(mock_catalogue)}\")\n",
        "print(mock_catalogue.columns())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `mock_catalogue` object is a table containing the positions, velocities, and other properties of the mock galaxies. You can inspect its columns as shown above.\n",
        "\n",
        "### c) Computing Correlation Functions (wp and xi)\n",
        "\n",
        "HODDIES can compute various two-point correlation functions from your mock catalogues. The `compute_observable()` method is used for this purpose.\n",
        "\n",
        "We will compute the projected correlation function ($w_p$) and the redshift-space multipoles (monopole $\\xi_0$ and quadrupole $\\xi_2$). The settings for these computations are defined in the `2PCF_settings` section of your parameter file. We will use the default settings from the loaded `test_fit_param_QSO.yaml`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the correlation functions\n",
        "# The fit_type is set to 'wp+xi' in test_fit_param_QSO.yaml, so it will compute both.\n",
        "computed_cf = hod_obj.get_wp(mock_catalogue, tracer_name='QSO')\n",
        "\n",
        "print(\"Correlation functions computed successfully!\")\n",
        "print(\"Keys available in computed_cf:\", computed_cf.keys())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `computed_cf` dictionary will contain the results, typically `'wp'` for projected correlation function and `'xi'` for redshift-space multipoles. The `'xi'` entry will itself be a dictionary containing `'xi0'` and `'xi2'`.\n",
        "\n",
        "### d) Plotting the Outputs\n",
        "\n",
        "Finally, let's visualize the computed correlation functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract results for plotting\n",
        "rp_bins = computed_cf['wp']['r'] # Radial bins for wp\n",
        "wp = computed_cf['wp']['wprp']\n",
        "\n",
        "s_bins = computed_cf['xi']['r'] # Radial bins for xi multipoles\n",
        "xi0 = computed_cf['xi']['xi0']\n",
        "xi2 = computed_cf['xi']['xi2']\n",
        "\n",
        "# Plot wp\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.errorbar(rp_bins, wp, fmt='o-', label='$w_p(r_p)$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel(r'$r_p$ (Mpc/h)')\n",
        "plt.ylabel(r'$w_p(r_p)$')\n",
        "plt.title('Projected Correlation Function')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot xi multipoles\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.errorbar(s_bins, xi0, fmt='o-', label=r'$\\xi_0(s)$')\n",
        "plt.errorbar(s_bins, xi2, fmt='s-', label=r'$\\xi_2(s)$')\n",
        "plt.xscale('log')\n",
        "plt.xlabel(r'$s$ (Mpc/h)')\n",
        "plt.ylabel(r'$\\xi(s)$')\n",
        "plt.title('Redshift-Space Correlation Function Multipoles')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Minimization Procedure and Results\n",
        "\n",
        "HODDIES provides functionality to perform a minimization procedure to find the best-fit HOD parameters by comparing the mock galaxy correlation functions with observed data. This typically involves an optimization algorithm that iteratively adjusts HOD parameters to minimize a $\\chi^2$ value.\n",
        "\n",
        "### a) Describing the Minimization Procedure\n",
        "\n",
        "The minimization in HODDIES often leverages techniques like Gaussian Process Regression (GPR) coupled with Markov Chain Monte Carlo (MCMC) sampling. The general idea is:\n",
        "\n",
        "1.  **Define Priors**: Specify the allowed ranges for each HOD parameter in your parameter file under the `fit_param -> priors` section.\n",
        "2.  **Generate Training Samples (GPR)**: If `generate_training_sample` is `True` in `fit_param`, the code will generate a set of HOD parameter combinations (training points) within the defined priors. For each training point, it generates mock catalogues and computes their correlation functions. This information is used to train a Gaussian Process (GP) model, which acts as a fast emulator of the HOD model.\n",
        "3.  **Minimization/Sampling (MCMC)**: An MCMC sampler (e.g., `emcee` or `zeus`) is then used to explore the parameter space, guided by the GP model. The sampler aims to find the set of HOD parameters that best fit the observational data by minimizing a $\\chi^2$ likelihood. The `func_aq` (acquisition function) parameter in `fit_param` determines how new sampling points are chosen to improve the GP model.\n",
        "\n",
        "Key parameters for the minimization are set in the `fit_param` section of your YAML file, including `nb_real`, `fit_name`, `dir_output_fit`, `fit_type`, `sampling_type`, `N_trainning_points`, `n_calls`, `sampler`, `n_iter`, `nwalkers`, and `priors`.\n",
        "\n",
        "### b) How to Run the Minimization\n",
        "\n",
        "To run the minimization, you use the `minimize_hod()` method of the `HOD` object. Before running, ensure your parameter file is correctly configured with the `fit_param` section, including appropriate priors and settings for the minimizer and sampler.\n",
        "\n",
        "Let's re-initialize the HOD object (if not already done) and then run a simplified minimization example. For a real fit, `maxiter` and `n_calls` would be significantly larger.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-initialize HOD object (if running this cell independently)\n",
        "# z_sim = 0.95 # From previous section\n",
        "# path_to_abacus = '/global/cfs/cdirs/desi/cosmosim/Abacus' # From previous section\n",
        "# param_file_path = 'HODDIES/desi/test_fit_param_QSO.yaml' # From previous section\n",
        "# hod_obj = HOD(\n",
        "#     path_to_abacus_sim=path_to_abacus,\n",
        "#     param_file=param_file_path,\n",
        "#     hcat={\n",
        "#         'Abacus': {\n",
        "#             'sim_name': 'AbacusSummit_highbase_c000_ph100',\n",
        "#             'z_simu': z_sim\n",
        "#         }\n",
        "#     }\n",
        "# )\n",
        "\n",
        "print(\"Running a simplified minimization (this might take some time for real runs)...\")\n",
        "\n",
        "# Run the minimizer\n",
        "# We set a very small number of iterations for demonstration purposes.\n",
        "# For a real analysis, n_calls and n_iter (if using MCMC) would be much larger.\n",
        "# The save_fn parameter specifies the name of the file where the fit results will be saved.\n",
        "result_minimization = hod_obj.minimize_hod(\n",
        "    minimizer_options={\n",
        "        \"maxiter\": 2, # Very small for example, increase for real runs\n",
        "        \"popsize\": 2, \n",
        "        'xtol':1e-6\n",
        "    },\n",
        "    save_fn='tutorial_fit_result',\n",
        "    load_cov_jk=False, # Use jackknife covariance if True, otherwise assumes precomputed cov.\n",
        "    add_poisson_noise=False # Add Poisson noise to mocks\n",
        ")\n",
        "\n",
        "print(\"Minimization completed!\")\n",
        "print(\"Best-fit parameters (raw output):\")\n",
        "print(result_minimization['x'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `result_minimization` object (or the saved `.npy` file) contains the best-fit parameters and other information from the minimization. The `x` key typically holds the array of best-fit HOD parameters.\n",
        "\n",
        "To see the best-fit parameters in a more readable dictionary format, you can map them back to their names using the `priors` keys:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the names of the parameters from the priors\n",
        "param_names = hod_obj.args['fit_param']['priors']['QSO'].keys()\n",
        "\n",
        "# Map the best-fit values to their parameter names\n",
        "best_fit_params_dict = dict(zip(param_names, result_minimization['x']))\n",
        "\n",
        "print(\"Best-fit HOD Parameters:\")\n",
        "for param, value in best_fit_params_dict.items():\n",
        "    print(f\"{param}: {value:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c) Plotting the Results\n",
        "\n",
        "HODDIES provides a convenient method, `plot_bf_data()`, to visualize the best-fit HOD model's correlation functions against the observed data. This function takes the path to the best-fit file (saved by `minimize_hod`) and can plot the mocks generated with the best-fit parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the best-fit results\n",
        "# The bf_file parameter should point to the .npy file saved by minimize_hod\n",
        "# In this example, it's 'tutorial_fit_result.npy'\n",
        "fig = hod_obj.plot_bf_data(\n",
        "    bf_file='tutorial_fit_result.npy',\n",
        "    fix_seed=10, # Use a fixed seed for mock generation for consistent plotting\n",
        "    add_no_vsmear=True, # Option to plot with and without velocity smearing\n",
        "    verbose=True # Print more details during plotting\n",
        ")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Tutorial complete! You have learned how to initialize HODDIES, create mock catalogues, compute correlation functions, and perform a basic minimization.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cosmodesi-2025_03",
      "language": "python",
      "name": "cosmodesi-2025_03"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
